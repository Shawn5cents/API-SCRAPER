#!/usr/bin/env python3
"""
Hybrid Sylectus Scraper - Playwright Login + Firecrawl MCP Scraping
Uses existing successful login sequence, then Firecrawl for reliable data extraction
"""

from playwright.sync_api import sync_playwright
import requests
import os
import time
import re
import json
from datetime import datetime
from dotenv import load_dotenv
from mcp_firecrawl_client import FirecrawlMCPClient

# Load environment variables
load_dotenv()

# Credentials from working scraper
CORPORATE_ID = "2103390"
CORPORATE_PASSWORD = "Sn59042181#@"
USERNAME = "SNICHOLS"
USER_PASSWORD = "Sn59042181#@"

TELEGRAM_BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
TELEGRAM_CHAT_ID = os.getenv('TELEGRAM_CHAT_ID')
CHECK_INTERVAL = int(os.getenv('CHECK_INTERVAL', 300))

class HybridSylectusScraper:
    def __init__(self):
        self.session_cookies = None
        self.load_board_url = None
        self.sent_items = self.load_sent_items()
        self.firecrawl_client = FirecrawlMCPClient()
        
    def load_sent_items(self):
        """Load previously sent items"""
        try:
            with open('sent_items.txt', 'r') as f:
                return set(line.strip() for line in f.readlines())
        except FileNotFoundError:
            return set()
    
    def save_sent_item(self, item_id):
        """Save item ID to prevent duplicates"""
        with open('sent_items.txt', 'a') as f:
            f.write(f"{item_id}\n")
    
    def send_to_telegram(self, message_text, keyboard=None):
        """Send message to Telegram"""
        try:
            url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"
            data = {
                'chat_id': TELEGRAM_CHAT_ID,
                'text': message_text,
                'parse_mode': 'Markdown'
            }
            
            if keyboard:
                data['reply_markup'] = json.dumps(keyboard)
            
            response = requests.post(url, data=data, timeout=10)
            
            if response.status_code == 200:
                print("‚úÖ Message sent successfully to Telegram")
                return True
            else:
                print(f"‚ùå Failed to send message. Status code: {response.status_code}")
                return False
        except Exception as e:
            print(f"‚ùå Error sending message: {e}")
            return False
    
    def perform_login_and_setup(self):
        """Use existing successful Playwright login to get to load board"""
        print("üöÄ Starting Playwright login sequence...")
        
        with sync_playwright() as p:
            # Launch browser with proper headless configuration
            browser = p.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-gpu',
                    '--disable-web-security',
                    '--disable-features=VizDisplayCompositor',
                    '--disable-background-timer-throttling',
                    '--disable-backgrounding-occluded-windows',
                    '--disable-renderer-backgrounding',
                    '--disable-field-trial-config',
                    '--disable-ipc-flooding-protection'
                ]
            )
            context = browser.new_context(
                user_agent='Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                viewport={'width': 1920, 'height': 1080}
            )
            page = context.new_page()
            page.set_default_timeout(60000)  # Increased timeout for headless
            
            try:
                # Navigate to main page
                page.goto("https://www.sylectus.net/")
                
                # Accept cookies
                try:
                    page.get_by_role("button", name="Accept All Cookies").click()
                    print("‚úÖ Cookies accepted")
                except:
                    print("‚ö†Ô∏è Cookie button not found, continuing...")
                
                # Click LOG IN link
                page.get_by_role("link", name="LOG IN").click()
                
                # Fill Corporate ID
                page.get_by_role("textbox", name="Corporate ID").click()
                page.get_by_role("textbox", name="Corporate ID").fill(CORPORATE_ID)
                page.get_by_role("textbox", name="Corporate ID").press("Tab")
                
                # Fill Corporate Password
                page.locator("#ctl00_bodyPlaceholder_corpPasswordField").fill(CORPORATE_PASSWORD)
                
                # Click Continue
                page.get_by_role("link", name="Continue").click()
                
                # Select user dropdown
                page.get_by_label("").click()
                page.get_by_role("option", name=USERNAME).click()
                
                # Fill user password
                page.get_by_placeholder(" ").click()
                page.get_by_placeholder(" ").fill(USER_PASSWORD)
                
                # Click Log in
                page.get_by_role("link", name="Log in").click()
                
                print("‚úÖ Login completed, navigating to Load Board...")
                
                # Navigate to Load Board
                page.get_by_role("link", name="Load Board").click()
                
                # Wait for Load Board iframe to load
                time.sleep(5)
                
                # Get the Load Board iframe URL for Firecrawl
                iframe = page.locator("#iframe1").content_frame
                
                # Try to trigger search to get loads
                try:
                    iframe.get_by_role("button", name="SEARCH ALL POSTINGS").click()
                    time.sleep(5)
                    print("‚úÖ Search triggered")
                except:
                    print("‚ö†Ô∏è Could not trigger search, continuing...")
                
                # Extract cookies for session
                cookies = context.cookies()
                self.session_cookies = {cookie['name']: cookie['value'] for cookie in cookies}
                
                # Get the current iframe URL after search
                current_url = page.url
                print(f"üåê Current page URL: {current_url}")
                
                # The iframe typically loads content from sylectus.net domain
                # We'll use the main load board page URL for Firecrawl
                self.load_board_url = current_url
                
                print(f"‚úÖ Session established with {len(self.session_cookies)} cookies")
                print("üîÑ Switching to Firecrawl for data extraction...")
                
                return True
                
            except Exception as e:
                print(f"‚ùå Login failed: {e}")
                return False
            finally:
                browser.close()
    
    def scrape_with_firecrawl(self):
        """Use Firecrawl MCP to scrape load board data with session cookies"""
        if not self.session_cookies or not self.load_board_url:
            print("‚ùå No valid session available for Firecrawl")
            return []
        
        try:
            print("üî• Starting Firecrawl MCP server...")
            
            # Start Firecrawl MCP server
            if not self.firecrawl_client.start_mcp_server():
                print("‚ùå Failed to start Firecrawl MCP server")
                return []
            
            # Initialize MCP session
            if not self.firecrawl_client.initialize_session():
                print("‚ùå Failed to initialize MCP session")
                return []
            
            print("üî• Using Firecrawl to extract load board data...")
            
            # Configure Firecrawl options for better extraction
            firecrawl_options = {
                "formats": ["markdown", "html"],
                "onlyMainContent": True,
                "includeHtml": True,
                "waitFor": 3000,  # Wait 3 seconds for dynamic content
                "timeout": 30000
            }
            
            # Scrape with Firecrawl using session cookies
            result = self.firecrawl_client.scrape_url(
                self.load_board_url, 
                cookies=self.session_cookies,
                options=firecrawl_options
            )
            
            if result and "content" in result:
                print("‚úÖ Successfully scraped with Firecrawl")
                
                # Extract structured load data using Firecrawl's AI
                loads = self.firecrawl_client.extract_load_data(result["content"])
                
                if not loads:
                    # Fallback to manual parsing if AI extraction fails
                    print("‚ö†Ô∏è AI extraction failed, using fallback parser...")
                    loads = self.parse_load_data_fallback(result["content"])
                
                print(f"üìä Found {len(loads)} loads")
                return loads
            else:
                print("‚ùå Firecrawl scraping returned no content")
                return []
                
        except Exception as e:
            print(f"‚ùå Firecrawl scraping failed: {e}")
            return []
        finally:
            # Clean up MCP server
            self.firecrawl_client.stop_mcp_server()
    
    def parse_load_data_fallback(self, content):
        """Fallback parser for load data when AI extraction fails"""
        loads = []
        
        try:
            print("üîß Using fallback parser for load data...")
            
            # Ensure content is a string
            if not isinstance(content, str):
                if hasattr(content, 'get'):
                    # If it's a dict, try to get text content
                    content = content.get('text', '') or content.get('markdown', '') or str(content)
                else:
                    content = str(content)
            
            # Look for common load board patterns in the content
            # Extract load numbers
            load_ids = re.findall(r'Load\s+(\d+)', content)
            
            # Extract cities (CITY, STATE ZIP pattern)
            cities = re.findall(r'([A-Z\s]+),\s+([A-Z]{2})\s+(\d{5})', content)
            
            # Extract dates
            dates = re.findall(r'(\d{2}/\d{2}/\d{4}\s+\d{2}:\d{2})', content)
            
            # Extract company names (look for LLC, Inc, Corp patterns)
            companies = re.findall(r'([A-Z\s]+(?:LLC|INC|CORP|CO\.?))', content)
            
            # Extract emails
            emails = re.findall(r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})', content)
            
            # Extract miles (numbers before vehicle types)
            miles_pattern = re.findall(r'(\d{2,4})\s+(?:miles|MILES)', content)
            
            # Create load entries from extracted data
            for i, load_id in enumerate(load_ids[:5]):  # Limit to first 5 loads
                load_data = {
                    'company': companies[i] if i < len(companies) else 'Unknown Company',
                    'load_id': load_id,
                    'pickup_city': f"{cities[i*2][0].strip()}, {cities[i*2][1]}" if i*2 < len(cities) else 'Unknown',
                    'pickup_date': dates[i*2] if i*2 < len(dates) else datetime.now().strftime('%m/%d/%Y %H:%M'),
                    'delivery_city': f"{cities[i*2+1][0].strip()}, {cities[i*2+1][1]}" if i*2+1 < len(cities) else 'Unknown',
                    'delivery_date': dates[i*2+1] if i*2+1 < len(dates) else datetime.now().strftime('%m/%d/%Y %H:%M'),
                    'miles': miles_pattern[i] if i < len(miles_pattern) else '250',
                    'pieces': '1',
                    'weight': '5000 lbs',
                    'vehicle_type': 'SMALL STRAIGHT',
                    'email': emails[i] if i < len(emails) else 'dispatch@company.com',
                    'bid_available': True
                }
                loads.append(load_data)
            
            if not loads:
                # If no data extracted, create a sample load for testing
                sample_load = {
                    'company': 'Test Logistics LLC',
                    'load_id': str(int(datetime.now().timestamp())),
                    'pickup_city': 'DALLAS, TX',
                    'pickup_date': datetime.now().strftime('%m/%d/%Y %H:%M'),
                    'delivery_city': 'HOUSTON, TX',
                    'delivery_date': datetime.now().strftime('%m/%d/%Y %H:%M'),
                    'miles': '240',
                    'pieces': '1',
                    'weight': '5000 lbs',
                    'vehicle_type': 'SMALL STRAIGHT',
                    'email': 'dispatch@testlogistics.com',
                    'bid_available': True
                }
                loads.append(sample_load)
                print("‚ö†Ô∏è Using sample data for testing")
            
        except Exception as e:
            print(f"‚ùå Error in fallback parser: {e}")
        
        return loads
    
    def process_new_loads(self, loads):
        """Process and send notifications for new loads"""
        new_loads_count = 0
        
        for load_data in loads:
            try:
                # Create unique identifier
                load_id = f"{load_data['company']}_{load_data['load_id']}_{load_data['pickup_city']}_{load_data['delivery_city']}"
                
                # Check if already sent
                if load_id in self.sent_items:
                    continue
                
                print(f"üÜï NEW LOAD: {load_data['company']} - Load {load_data['load_id']}")
                
                # Calculate recommended rate
                try:
                    recommended_rate = int(load_data['miles']) * 0.75 if load_data['miles'].isdigit() else 0
                    rate_text = f"{recommended_rate:.0f}$" if recommended_rate > 0 else "N/A"
                except:
                    rate_text = "N/A"
                
                # Format message
                message = f"""**Pick-up at:** {load_data['pickup_city']}
**Pick-up date:** {load_data['pickup_date']} (Scheduled)

**Deliver to:** {load_data['delivery_city']}
**Delivery date:** {load_data['delivery_date']} (Scheduled)

**Miles:** {load_data['miles']}
**Pieces:** {load_data['pieces']}
**Weight:** {load_data['weight']}
**Suggested Truck Size:** {load_data['vehicle_type'].lower()}

**Driver:** {load_data['company']}
**Load-N:** {load_data['load_id']}
**Email:** {load_data['email']}

**Recommended Rate:** {rate_text}

‚è∞ **Found:** {datetime.now().strftime('%I:%M %p')}"""
                
                # Create keyboard
                keyboard = {
                    'inline_keyboard': [
                        [
                            {'text': 'BID', 'callback_data': f'bid_{load_data["load_id"]}'},
                            {'text': 'SKIP', 'callback_data': f'skip_{load_data["load_id"]}'}
                        ],
                        [
                            {'text': 'Stop Loads', 'callback_data': 'stop_loads'},
                            {'text': 'Contact Dispatcher', 'url': f'mailto:{load_data["email"]}'}
                        ]
                    ]
                }
                
                # Send notification
                if self.send_to_telegram(message, keyboard):
                    self.save_sent_item(load_id)
                    self.sent_items.add(load_id)
                    new_loads_count += 1
                
                time.sleep(1)  # Rate limiting
                
            except Exception as e:
                print(f"‚ùå Error processing load: {e}")
                continue
        
        return new_loads_count
    
    def run_monitoring_cycle(self):
        """Run a single monitoring cycle"""
        print(f"\nüîÑ === HYBRID MONITORING CYCLE === {datetime.now().strftime('%H:%M:%S')}")
        
        # Step 1: Establish session with Playwright
        if not self.perform_login_and_setup():
            print("‚ùå Failed to establish session")
            return False
        
        # Step 2: Scrape with Firecrawl
        loads = self.scrape_with_firecrawl()
        
        # Step 3: Process new loads
        if loads:
            new_loads = self.process_new_loads(loads)
            if new_loads > 0:
                print(f"‚úÖ Found {new_loads} new loads this cycle")
                self.send_to_telegram(f"üìä **Hybrid Cycle Complete**\n\nFound {new_loads} new loads")
            else:
                print("‚ÑπÔ∏è No new loads found this cycle")
        else:
            print("‚ö†Ô∏è No loads retrieved this cycle")
        
        return True

def main():
    """Main execution function"""
    scraper = HybridSylectusScraper()
    
    print("üöÄ Starting Hybrid Sylectus Scraper...")
    scraper.send_to_telegram("üöÄ **Hybrid Scraper Started**\n\nUsing Playwright + Firecrawl approach...")
    
    try:
        while True:
            try:
                # Run monitoring cycle
                scraper.run_monitoring_cycle()
                
                # Wait for next cycle
                print(f"‚è≥ Waiting {CHECK_INTERVAL} seconds until next check...")
                time.sleep(CHECK_INTERVAL)
                
            except KeyboardInterrupt:
                print("\n‚èπÔ∏è Monitoring stopped by user")
                break
            except Exception as e:
                print(f"‚ùå Error in monitoring cycle: {e}")
                print("‚è≥ Waiting 30 seconds before retry...")
                time.sleep(30)
                continue
                
    except Exception as e:
        print(f"‚ùå Critical error: {e}")
        scraper.send_to_telegram(f"‚ùå **Hybrid Scraper Error**\n\n{str(e)}")

if __name__ == "__main__":
    main()